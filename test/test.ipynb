{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "RNN for slot filling\n",
    "dataSet Object\n",
    "by D. Hakkani-Tur\n",
    "modified by V. Chen\n",
    "\"\"\"\n",
    "# import re\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class dataSet(object):\n",
    "    \"\"\"\n",
    "      utterances with slot tags\n",
    "    \"\"\"\n",
    "    def __init__(self, dataFile, toggle, wordDictionary, tagDictionary, id2word, id2tag):\n",
    "        if toggle == 'train':\n",
    "            self.dataSet = readData(dataFile)\n",
    "        if toggle == 'val':\n",
    "            self.dataSet = readTest(dataFile, wordDictionary, tagDictionary, id2word, id2tag)\n",
    "        if toggle == 'test':\n",
    "            self.dataSet = readTest(dataFile, wordDictionary, tagDictionary, id2word, id2tag)\n",
    "\n",
    "    def getNum(self, numFile):\n",
    "        return readNum(numFile)\n",
    "\n",
    "    def getWordVocabSize(self):\n",
    "        return self.dataSet['wordVocabSize']\n",
    "\n",
    "    def getTagVocabSize(self):\n",
    "        return self.dataSet['tagVocabSize']\n",
    "\n",
    "    def getNoExamples(self):\n",
    "        return self.dataSet['uttCount']\n",
    "\n",
    "    def getExampleUtterance(self, index):\n",
    "        return self.dataSet['utterances'][index]\n",
    "\n",
    "    def getExampleTags(self, index):\n",
    "        return self.dataSet['tags'][index]\n",
    "\n",
    "    def getWordVocab(self):\n",
    "        return self.dataSet['word2id']\n",
    "\n",
    "    def getTagVocab(self):\n",
    "        return self.dataSet['tag2id']\n",
    "\n",
    "    def getIndex2Word(self):\n",
    "        return self.dataSet['id2word']\n",
    "\n",
    "    def getIndex2Tag(self):\n",
    "        return self.dataSet['id2tag']\n",
    "\n",
    "    def getTagAtIndex(self, index):\n",
    "        return self.dataSet['id2tag'][index]\n",
    "\n",
    "    def getWordAtIndex(self, index):\n",
    "        return self.dataSet['id2word'][index]\n",
    "\n",
    "    def getSample(self, batchSize):\n",
    "        inputs = {}\n",
    "        targets = {}\n",
    "        indices = np.random.randint(0, self.dataSet['uttCount'], size=batchSize)\n",
    "        for i in xrange(batchSize):\n",
    "            inputs[i] = self.dataSet['utterances'][indices[i]]\n",
    "            targets[i] = self.dataSet['tags'][indices[i]]\n",
    "        return inputs,targets\n",
    "\n",
    "\"\"\"\n",
    "   def encodeInput(self, encode_type, time_length):\n",
    "       from keras.preprocessing import sequence\n",
    "       # preprocessing by padding 0 until maxlen\n",
    "       pad_X = sequence.pad_sequences(trainData.dataSet['utterances'], maxlen=self.time_length, dtype='int32')\n",
    "       pad_y = sequence.pad_sequences(trainData.dataSet['tags'], maxlen=self.time_length, dtype='int32')\n",
    "       num_sample, max_len = np.shape(pad_X)\n",
    "\n",
    "       if encode_type == '1hot':\n",
    "           self.dataSet['utterances']\n",
    "\"\"\"\n",
    "\n",
    "def readHisData(dataFile):\n",
    "\n",
    "    # read the data sets\n",
    "    # each line has one utterance that contains tab separated utterance words and corresponding IOB tags\n",
    "    history = list()\n",
    "    utterances = list()\n",
    "    tags = list()\n",
    "\n",
    "    # reserving index 0 for padding\n",
    "    # reserving index 1 for unknown word and tokens\n",
    "    word_vocab_index = 2\n",
    "    tag_vocab_index = 2\n",
    "    word2id = {'<pad>': 0, '<unk>': 1}\n",
    "    tag2id = {'<pad>': 0, '<unk>': 1}\n",
    "    id2word = ['<pad>', '<unk>']\n",
    "    id2tag = ['<pad>', '<unk>']\n",
    "\n",
    "    utt_count = 0\n",
    "    for line in open(dataFile, 'r'):\n",
    "        d = line.split('\\t')\n",
    "        his = d[0].strip()\n",
    "        utt = d[1].strip()\n",
    "        t = d[2].strip()\n",
    "        print 'his: %s, utt: %s, tags: %s'%(his, utt, t)\n",
    "\n",
    "        temp_his = list()\n",
    "        temp_utt = list()\n",
    "        temp_tags = list()\n",
    "        if his != '':\n",
    "            myhis = his.split()\n",
    "        mywords = utt.split(' ')\n",
    "        mytags = t.split(' ')\n",
    "        # now add the words and tags to word and tag dictionaries\n",
    "        # also save the word and tag sequence in training data sets\n",
    "        for i in xrange(len(mywords)):\n",
    "            if mywords[i] not in word2id:\n",
    "                word2id[mywords[i]] = word_vocab_index\n",
    "                id2word.append(mywords[i])\n",
    "                word_vocab_index += 1\n",
    "            if mytags[i] not in tag2id:\n",
    "                tag2id[mytags[i]] = tag_vocab_index\n",
    "                id2tag.append(mytags[i])\n",
    "                tag_vocab_index += 1\n",
    "            temp_utt.append(word2id[mywords[i]])\n",
    "            temp_tags.append(tag2id[mytags[i]])\n",
    "        if his != '':\n",
    "            for i in xrange(len(myhis)):\n",
    "                temp_his.append(word2id[myhis[i]])\n",
    "        utt_count += 1\n",
    "        history.append(temp_his)\n",
    "        utterances.append(temp_utt)\n",
    "        tags.append(temp_tags)\n",
    "\n",
    "    data = {'history': history, 'utterances': utterances, 'tags': tags, 'uttCount': utt_count, 'id2word':id2word, 'id2tag':id2tag, 'wordVocabSize' : word_vocab_index, 'tagVocabSize': tag_vocab_index, 'word2id': word2id, 'tag2id':tag2id}\n",
    "    return data\n",
    "\n",
    "def readData(dataFile):\n",
    "\n",
    "    # read the data sets\n",
    "    # each line has one utterance that contains tab separated utterance words and corresponding IOB tags\n",
    "    # if the input is multiturn session data, the flag following the IOB tags is 1 (session start) or 0 (not session start)\n",
    "\n",
    "    utterances = list()\n",
    "    tags = list()\n",
    "    starts = list()\n",
    "    startid = list()\n",
    "\n",
    "    # reserving index 0 for padding\n",
    "    # reserving index 1 for unknown word and tokens\n",
    "    word_vocab_index = 2\n",
    "    tag_vocab_index = 2\n",
    "    word2id = {'<pad>': 0, '<unk>': 1}\n",
    "    tag2id = {'<pad>': 0, '<unk>': 1}\n",
    "    id2word = ['<pad>', '<unk>']\n",
    "    id2tag = ['<pad>', '<unk>']\n",
    "\n",
    "    utt_count = 0\n",
    "    temp_startid = 0\n",
    "    for line in open(dataFile, 'r'):\n",
    "        d=line.split('\\t')\n",
    "        utt = d[0].strip()\n",
    "        t = d[1].strip()\n",
    "        if len(d) > 2:\n",
    "            start = np.bool(int(d[2].strip()))\n",
    "            starts.append(start)\n",
    "            if start:\n",
    "                temp_startid = utt_count\n",
    "            startid.append(temp_startid)\n",
    "        #print 'utt: %s, tags: %s' % (utt,t)\n",
    "\n",
    "        temp_utt = list()\n",
    "        temp_tags = list()\n",
    "        mywords = utt.split()\n",
    "        mytags = t.split()\n",
    "        if len(mywords) != len(mytags):\n",
    "            print mywords\n",
    "            print mytags\n",
    "        # now add the words and tags to word and tag dictionaries\n",
    "        # also save the word and tag sequence in training data sets\n",
    "        for i in xrange(len(mywords)):\n",
    "            if mywords[i] not in word2id:\n",
    "                word2id[mywords[i]] = word_vocab_index\n",
    "                id2word.append(mywords[i])\n",
    "                word_vocab_index += 1\n",
    "            if mytags[i] not in tag2id:\n",
    "                tag2id[mytags[i]] = tag_vocab_index\n",
    "                id2tag.append(mytags[i])\n",
    "                tag_vocab_index += 1\n",
    "            temp_utt.append(word2id[mywords[i]])\n",
    "            temp_tags.append(tag2id[mytags[i]])\n",
    "        utt_count += 1\n",
    "        utterances.append(temp_utt)\n",
    "        tags.append(temp_tags)\n",
    "\n",
    "    data = {'start': starts, 'startid': startid, 'utterances': utterances, 'tags': tags, 'uttCount': utt_count, 'id2word':id2word, 'id2tag':id2tag, 'wordVocabSize' : word_vocab_index, 'tagVocabSize': tag_vocab_index, 'word2id': word2id, 'tag2id':tag2id}\n",
    "    return data\n",
    "\n",
    "def readTest(testFile, word2id, tag2id, id2word, id2tag):\n",
    "\n",
    "    utterances = list()\n",
    "    tags = list()\n",
    "    starts = list()\n",
    "    startid = list()\n",
    "\n",
    "    utt_count = 0\n",
    "    temp_startid = 0\n",
    "    for line in open(testFile, 'r'):\n",
    "        d=line.split('\\t')\n",
    "        utt = d[0].strip()\n",
    "        t = d[1].strip()\n",
    "        if len(d) > 2:\n",
    "            start = np.bool(int(d[2].strip()))\n",
    "            starts.append(start)\n",
    "            if start:\n",
    "                temp_startid = utt_count\n",
    "            startid.append(temp_startid)\n",
    "        #print 'utt: %s, tags: %s' % (utt,t)\n",
    "\n",
    "        temp_utt = list()\n",
    "        temp_tags = list()\n",
    "        mywords = utt.split()\n",
    "        mytags = t.split()\n",
    "        # now add the words and tags to word and tag dictionaries\n",
    "        # also save the word and tag sequence in training data sets\n",
    "        for i in xrange(len(mywords)):\n",
    "            if mywords[i] not in word2id:\n",
    "                temp_utt.append(1) #i.e. append unknown word\n",
    "            else:\n",
    "                temp_utt.append(word2id[mywords[i]])\n",
    "            if mytags[i] not in tag2id:\n",
    "                temp_tags.append(1)\n",
    "            else:\n",
    "                temp_tags.append(tag2id[mytags[i]])\n",
    "        utt_count += 1\n",
    "        utterances.append(temp_utt)\n",
    "        tags.append(temp_tags)\n",
    "        word_vocab_size = len(word2id)\n",
    "\n",
    "    data = {\n",
    "        'start': starts,\n",
    "        'startid': startid,\n",
    "        'utterances': utterances,\n",
    "        'tags': tags,\n",
    "        'uttCount': utt_count,\n",
    "        'wordVocabSize': word_vocab_size,\n",
    "        'id2word': id2word,\n",
    "        'id2tag': id2tag}\n",
    "    return data\n",
    "\n",
    "def readNum(numFile):\n",
    "\n",
    "    numList = map(int, file(numFile).read().strip().split())\n",
    "    totalList = list()\n",
    "    cur = 0\n",
    "    for num in numList:\n",
    "        cur += num + 1\n",
    "        totalList.append(cur)\n",
    "    return numList, totalList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atis-2.dev.iob           atis-2.train.w-intent.iob  atis.train.iob\r\n",
      "atis-2.dev.w-intent.iob  atis.test.iob              atis.train.w-intent.iob\r\n",
      "atis-2.train.iob         atis.test.w-intent.iob     sample.iob\r\n"
     ]
    }
   ],
   "source": [
    "ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "emptyVocab = {}\n",
    "emptyIndex = list()\n",
    "time_length = 48\n",
    "\n",
    "trainData = dataSet('data/atis-2.train.w-intent.iob','train', emptyVocab, emptyVocab, emptyIndex, emptyIndex)\n",
    "testData = dataSet('data/atis.test.w-intent.iob', 'test', trainData.getWordVocab(), trainData.getTagVocab(),trainData.getIndex2Word(),trainData.getIndex2Tag())        \n",
    "\n",
    "pad_X_test = sequence.pad_sequences(testData.dataSet['utterances'], maxlen=time_length, dtype='int32', padding='pre')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('word: ', ['BOS', 'i', 'would', 'like', 'a', 'flight', 'traveling', 'one', 'way', 'from', 'phoenix', 'to', 'san', 'diego', 'on', 'april', 'first', 'EOS'])\n",
      "('Sem padding: ', [2, 3, 187, 70, 51, 52, 480, 119, 120, 7, 85, 5, 33, 79, 26, 446, 116, 12])\n",
      "('Com padding: ', array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "         0,   0,   0,   0,   2,   3, 187,  70,  51,  52, 480, 119, 120,\n",
      "         7,  85,   5,  33,  79,  26, 446, 116,  12], dtype=int32))\n"
     ]
    }
   ],
   "source": [
    "idx2w=  testData.getIndex2Word()\n",
    "print('word: ', [idx2w[x] for x in testData.dataSet['utterances'][3]])\n",
    "print('Sem padding: ',testData.dataSet['utterances'][3])\n",
    "print('Com padding: ', pad_X_test[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mdata\u001b[0m/  \u001b[01;34mexperiment\u001b[0m/  __init__.py  \u001b[01;34mprogram\u001b[0m/  README.md  \u001b[01;34mscript\u001b[0m/  \u001b[01;34mtest\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment/mdl.w-intent/150-0.50/blstm+emb.10.h5\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'hidden_size': 150,\n",
    "    'dropout': '0.50',\n",
    "    'model_arch':'blstm+emb',\n",
    "    'num_iter': 10   \n",
    "}\n",
    "\n",
    "path_model = 'experiment/mdl.w-intent/{hidden_size}-{dropout}/{model_arch}.{num_iter}.h5'.format(\n",
    "    hidden_size=config['hidden_size'],\n",
    "    dropout=config['dropout'],\n",
    "    model_arch=config['model_arch'],\n",
    "    num_iter=config['num_iter'],    \n",
    ")\n",
    "print(path_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(path_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 48)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 48, 100)      87100       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 48, 150)      150600      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 48, 150)      150600      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 48, 300)      0           lstm_1[0][0]                     \n",
      "                                                                 lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 48, 300)      0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 48, 143)      43043       dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 431,343\n",
      "Trainable params: 431,343\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def predict(text):\n",
    "    slots = {}\n",
    "    intent = ''\n",
    "    query = text\n",
    "    mywords = ' BOS '+ text + ' EOS '\n",
    "\n",
    "    tagDict = trainData.dataSet['id2tag']\n",
    "    word2idx = trainData.getWordVocab()\n",
    "    idx2w =  testData.getIndex2Word()\n",
    "\n",
    "    encoded = np.array([word2idx[x] for x in mywords.split()])\n",
    "    encoded = encoded[np.newaxis, :]\n",
    "    X = sequence.pad_sequences(encoded, maxlen=time_length, dtype='int32', padding='pre')\n",
    "\n",
    "    probability = model.predict(X)\n",
    "    prediction = np.argmax(probability, axis=2)            \n",
    "\n",
    "    for wid, tid in zip(X.flatten(), prediction.flatten()):\n",
    "        key =tagDict[tid]    \n",
    "        value = word=idx2w[wid]\n",
    "\n",
    "        if value == 'EOS':\n",
    "            intent = key\n",
    "        elif wid != 0:    \n",
    "            slots[key] = value \n",
    "\n",
    "    return {\n",
    "        'query':query,\n",
    "        'entries': slots, \n",
    "        'intent': intent\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entries': {'B-depart_time.time': '755',\n",
       "  'B-flight_time': 'arrival',\n",
       "  'B-fromloc.city_name': 'washington',\n",
       "  'I-depart_time.time': 'am',\n",
       "  'I-flight_time': 'time',\n",
       "  'I-fromloc.city_name': 'francisco',\n",
       "  'O': 'leaving'},\n",
       " 'intent': 'atis_flight_time',\n",
       " 'query': 'what is the arrival time in san francisco for the 755 am flight leaving washington'}"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('what is the arrival time in san francisco for the 755 am flight leaving washington')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entries': {'B-fromloc.city_name': 'baltimore',\n",
       "  'B-round_trip': 'round',\n",
       "  'B-toloc.city_name': 'dallas',\n",
       "  'I-round_trip': 'trip',\n",
       "  'O': 'to'},\n",
       " 'intent': 'atis_flight',\n",
       " 'query': 'i want to fly from baltimore to dallas round trip'}"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('i want to fly from baltimore to dallas round trip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
